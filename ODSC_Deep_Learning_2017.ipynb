{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".expo {\n",
       "  line-height: 150%;\n",
       "}\n",
       "\n",
       ".visual {\n",
       "  width: 600px;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"\"\"\n",
    "<style>\n",
    ".expo {\n",
    "  line-height: 150%;\n",
    "}\n",
    "\n",
    ".visual {\n",
    "  width: 600px;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning from Scratch using Python \n",
    "\n",
    "Seth Weidman\n",
    "\n",
    "11/04/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# To begin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nah..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "This talk will have two parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 1: Neural Nets from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll implement a basic neural net with one hidden layer, from scratch, and use mathematical principles to get the backpropogation right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll show that this same framework can be used to learn MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 2: Transitioning to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll transition to Deep Learning by changing our mental model of neural nets to be that they contain \"layers\" which pass information backwards and forwards between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll show how this framework can be used to construct arbitrarily deep neural networks, and show how these can learn MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Neural Nets from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "We've all seen diagrams like the following in the context of neural nets:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 1: Neural Nets from Scratch\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_v3.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Many don't fully understand what is going on in this diagram. This talk will attempt to rectify that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why neural nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Let's suppose we need a function that  that can learn a complicated relationship between inputs and outputs:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df, X, Y = generate_x_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Neural Nets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  X3  y\n",
       "0   1   0   0  1\n",
       "1   0   1   0  0\n",
       "2   1   1   0  1\n",
       "3   0   0   0  0\n",
       "4   1   0   1  0\n",
       "5   0   0   1  0\n",
       "6   1   1   1  1\n",
       "7   0   1   1  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "So: we have eight observations, and a complex relationship between inputs and outputs. Now, let's build a neural net that can \"learn\" this relationship.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Neural Nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "How to even begin? Well, let's look at this from as high a level as possible and then progressively dive deeper. First, we want a function $N$ that--based on the data from before--maps inputs to outputs properly, that is:\n",
    "</div>\n",
    "\n",
    "$$ N(1, 0, 0) = 1 $$\n",
    "$$ N(0, 1, 0) = 1 $$\n",
    "$$ N(1, 1, 0) = 1 $$\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "First, let's observe that logistic regression can't do this. That is, there are no parameters $b$, $w_1$, $w_2$, and $w_3$ such that:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$N(x_1, x_2, x_3) = \\frac{1}{1 + e^{b + w_1 * x_1 + w_2 * x_2 + w_3 * x_3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "This is true for the same reason that logistic regression cannot learn XOR. Our problem is a three dimensional problem since we have three features, but you can easily see in two dimensions that the space is not linearly separable:\n",
    "</div>\n",
    "\n",
    "<div class=\"visual\">\n",
    "  <img src=\"img/xor.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Of course, we *could* manually do feature engineering...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... but who likes feature engineering???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Let's have the computer do the feature engineering for us, via a neural net learning hidden features!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's make a prediction using a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal will be to:\n",
    "\n",
    "* Start with our three original features.\n",
    "* Transform them into four \"intermediate features\" using logistic-regression-like transformation.\n",
    "* Take these four \"intermediate features\" and use _them_ to predict our final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we transform our original three features into an intermediate, or hidden layer? Let's call our intermediate features $a_1$, $a_2$, $a_3$, and $a_4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want $a_1$, for example, to be a linear combination of $x_1$, $x_2$, and $x_3$ - that is, we want some weights $v_{11}$, $v_{12}$, and $v_{13}$ so that:\n",
    "\n",
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "\n",
    "and similarly for $a_2$, $a_3$, and $a_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A way to concisely express this is to define your features as a vector\n",
    "\n",
    "$$ X = \\begin{bmatrix}x_1 & x_2 & x_3\\end{bmatrix} $$\n",
    "\n",
    "This should be intuitive, since $X$ is already a row in your data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, multiplying this vector by a matrix $V$:\n",
    "\n",
    "$$ V = \\begin{bmatrix}v_{11} & v_{12} & v_{13} & v_{14} \\\\\n",
    "                      v_{21} & v_{22} & v_{23} & v_{24} \\\\\n",
    "                      v_{31} & v_{32} & v_{33} & v_{34}\n",
    "                      \\end{bmatrix} $$\n",
    "\n",
    "gives us what we want, since \"$A = X * V$\" is equivalent to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "$$ a_2 = x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} $$\n",
    "$$ a_3 = x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} $$\n",
    "$$ a_4 = x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} $$\n",
    "\n",
    "which is what we want in order to get four intermediate features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's code this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[1 0 0]]\n",
      "The dimensions are 1 row and 3 columns\n"
     ]
    }
   ],
   "source": [
    "x = np.array(X[0], ndmin=2)\n",
    "array_print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.36  0.3  -0.63 -0.34]\n",
      " [-1.03  1.26  0.41  1.2 ]\n",
      " [-1.07 -0.68  0.08  1.04]]\n",
      "The dimensions are 3 rows and 4 columns\n"
     ]
    }
   ],
   "source": [
    "V = np.random.randn(3, 4)\n",
    "array_print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.36  0.3  -0.63 -0.34]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "A = np.dot(x, V)\n",
    "array_print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_first_layer.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We're going to use a classic, easy-to-understand activation function, though one that is not often used in cutting-edge applications: the sigmoid function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ B = \\sigma(A) $$ or\n",
    "\n",
    "$$ b_1 = \\sigma(a_1) $$\n",
    "$$ b_2 = \\sigma(a_2) $$\n",
    "$$ b_3 = \\sigma(a_3) $$\n",
    "$$ b_4 = \\sigma(a_4) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.59  0.58  0.35  0.42]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "B = sigmoid(A)\n",
    "array_print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_first_sigmoid.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll multiply these \"sigmoided\" results by another matrix $W$ to get a single output. Since we want to transform 4 features down into 1, we can use a 4 x 1 matrix:\n",
    "\n",
    "$$ W = \\begin{bmatrix}w_{11} \\\\\n",
    "                      w_{21} \\\\\n",
    "                      w_{31} \\\\\n",
    "                      w_{41}\n",
    "                      \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And since we want the result to be:\n",
    "\n",
    "$$ c_1 = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to writing:\n",
    "\n",
    "$$ C = B * W $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "c_1 \\end{bmatrix} = \n",
    "\\begin{bmatrix}b_1 &\n",
    "                  b_2 &\n",
    "                  b_3 &\n",
    "                  b_4\n",
    "                  \\end{bmatrix} * \n",
    "\\begin{bmatrix}w_{11} \\\\\n",
    "               w_{21} \\\\\n",
    "               w_{31} \\\\\n",
    "               w_{41}\n",
    "               \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can simply code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.47]\n",
      " [ 0.94]\n",
      " [ 0.56]\n",
      " [-1.2 ]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(4, 1)\n",
    "array_print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.04]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "C = np.dot(B, W)\n",
    "array_print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_second_layer.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4: sigmoid this to make a final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we want:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ p_1 = \\sigma(c_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can simply code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.49]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "P = sigmoid(C)\n",
    "array_print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_final_prediction.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 5: compute the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we'll compute mean squared error loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = \\frac{1}{2}(y - P)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.13]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "y = np.array(Y[0], ndmin=2)\n",
    "L = 0.5 * (y - P) ** 2\n",
    "array_print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_loss.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have made our prediction and computed our loss, $L$. Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall: each \"step\" is just a function applied to some input that results in some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we write out what we just did in terms of mathematical functions, we could write it as:\n",
    "\n",
    "\\begin{align}\n",
    "A &= a(x, V) \\\\\n",
    "B &= b(A) \\\\\n",
    "C &= c(B, W) \\\\\n",
    "P &= p(C) \\\\\n",
    "L &= l(P)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, say we have a neural net with just one hidden layer. We could write the loss of a neural net on a given observation $ x $ as:\n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we _want_ to change the weights in such a way that the loss will be reduced during the next iteration. The equations:\n",
    "\n",
    "$$ W = W - \\frac{\\partial l}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial l}{\\partial V}$$\n",
    "\n",
    "do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that this \"makes sense\":\n",
    "\n",
    "* If $\\frac{\\partial l}{\\partial W}$ is a positive number, then we want to _decrease_ the weight, since increasing the weight would _increase_ our loss. That is exactly what the equation $ W = W - \\frac{\\partial l}{\\partial W}$ does.\n",
    "* Similarly, if $\\frac{\\partial l}{\\partial W}$ is a negative number, then we want to _increase_ the weight, since increasing the weight would _decrease_ our loss. In both cases, the equation $ W = W - \\frac{\\partial l}{\\partial W}$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we want to make our neural net smarter by updating its weights. We've see that to do that, we need to compute $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial V}$. How do we do this?\n",
    "\n",
    "Well, we know that \n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our good friend the chain rule tells us that: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial W}  $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial B} * \\frac{\\partial b}{\\partial A} * \\frac{\\partial a}{\\partial V}  $$\n",
    "\n",
    "Each one of these partial derivatives turns out to be simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, let's compute:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial P} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since \n",
    "\n",
    "$$ L = l(P) = \\frac{1}{2}(y - P)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial P} = -(y - P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.51]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "dLdP = -(y - P)\n",
    "array_print(dLdP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_loss_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, let's compute:\n",
    "\n",
    "$$ \\frac{\\partial p}{\\partial C} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that:\n",
    "\n",
    "$$ P = \\begin{bmatrix} p_1 \\end{bmatrix} = p(c) = \\sigma(c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A digression on the sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) * (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if\n",
    "\n",
    "$$ p(c) = \\sigma(c) $$\n",
    "\n",
    "then\n",
    "\n",
    "$$ p'(c) = \\sigma(c) * (1 - \\sigma(c)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.25]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "dPdC = sigmoid(C) * (1-sigmoid(C))\n",
    "array_print(dPdC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_prediction_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next we want to compute:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "C &= \\begin{bmatrix} c_1 \\end{bmatrix} \\\\ \n",
    "&= c(W) \\\\\n",
    "&= w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now recall that by $ \\frac{\\partial c}{\\partial W} $ we mean:\n",
    "\n",
    "$$ \\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But since \n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ \\frac{\\partial c}{\\partial w_{11}} $, for example, is just $b_1$, $ \\frac{\\partial c}{\\partial w_{21}} $ is just $b_2$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus,\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is just $ B^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.59]\n",
      " [ 0.58]\n",
      " [ 0.35]\n",
      " [ 0.42]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dCdW = B.T\n",
    "array_print(dCdW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that this has the same dimensions as `W`, which is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing $\\frac{\\partial L}{\\partial W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now computing $\\frac{\\partial L}{\\partial W}$ is simply a matter of doing the matrix multiplications, which again, by the chain rule, will actually cause the weights to be updated in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.075]\n",
      " [-0.073]\n",
      " [-0.044]\n",
      " [-0.053]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dLdW = np.dot(dCdW, dLdP * dPdC)\n",
    "array_print(dLdW, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By the same logic that we applied in Step 3, since:\n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} = B^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And again, since:\n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$\n",
    "\n",
    "We have:\n",
    "    \n",
    "$$ \\frac{\\partial c}{\\partial B} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial b_1} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_2} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_3} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_4}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}w_{11} \\\\\n",
    "                  w_{21} \\\\\n",
    "                  w_{31} \\\\\n",
    "                  w_{41}\n",
    "                  \\end{bmatrix} = W^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So coding this up simply gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.47  0.94  0.56 -1.2 ]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dCdB = W.T\n",
    "array_print(dCdB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_c_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, we want to compute:\n",
    "\n",
    "$$ \\frac{\\partial b}{\\partial A} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But since:\n",
    "\n",
    "$$ B = b(A) = \\sigma(A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is really just shorthand for:\n",
    "\n",
    "$$ B = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4 \\end{bmatrix} = \\begin{bmatrix} \\sigma(a_1) \\\\ \\sigma(a_2) \\\\ \\sigma(a_3) \\\\ \\sigma(a_4) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we know that:\n",
    "    \n",
    "$$ \\sigma'(A) = \\sigma(A) * (1 - \\sigma(A)) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then:\n",
    "    \n",
    "$$ \\frac{\\partial b}{\\partial A} = \\begin{bmatrix} \\sigma(a_1) * (1 - \\sigma(a_1) \\\\ \n",
    "\\sigma(a_2) * (1 - \\sigma(a_2) \\\\ \n",
    "\\sigma(a_3) * (1 - \\sigma(a_3) \\\\\n",
    "\\sigma(a_4) * (1 - \\sigma(a_4) \\end{bmatrix} = \\sigma(A) * (1 - \\sigma(A))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.24  0.24  0.23  0.24]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dBdA = sigmoid(A) * (1-sigmoid(A))\n",
    "array_print(dBdA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_b_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, we want to compute the most involved of our partial derivatives:\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial V} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recalling that:\n",
    "\n",
    "$$ a(X, V) = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But $ a(X, V) $ is itself shorthand for the equations:\n",
    "\n",
    "$$ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $$\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$\n",
    "$$ x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} = a_3 $$\n",
    "$$ x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} = a_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So $ \\frac{\\partial a}{\\partial V} $ is really shorthand for:\n",
    "\n",
    "$$ \\begin{bmatrix}\\frac{\\partial a}{\\partial v_{11}} & \\frac{\\partial a}{\\partial v_{12}} & \\frac{\\partial a}{\\partial v_{13}} & \\frac{\\partial a}{\\partial v_{14}} \\\\\n",
    "\\frac{\\partial a}{\\partial v_{21}} & \\frac{\\partial a}{\\partial v_{22}} & \\frac{\\partial a}{\\partial v_{23}} & \\frac{\\partial a}{\\partial v_{24}} \\\\\n",
    "\\frac{\\partial a}{\\partial v_{31}} & \\frac{\\partial a}{\\partial v_{32}} & \\frac{\\partial a}{\\partial v_{33}} & \\frac{\\partial a}{\\partial v_{34}} \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But, note that focusing on just $a_1$ for example:\n",
    "\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{11}} = x_1 $$\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{21}} = x_2 $$\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{31}} = x_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since again, $ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "whereas for $a_2$ and $a_3$\n",
    "\n",
    "$$ \\frac{\\partial a_2}{\\partial v_{11}} = 0 $$\n",
    "$$ \\frac{\\partial a_3}{\\partial v_{11}} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since, for example:\n",
    "\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if we write: \n",
    "    \n",
    "$$ A = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then $\\frac{\\partial a}{\\partial V}$ ends up being:\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial V} = \\begin{bmatrix}\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which in terms of the matrix multiplication that results is the same as writing just:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{\\partial a}{\\partial V} = X^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is of course easy to code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[1]\n",
      " [0]\n",
      " [0]]\n",
      "The dimensions are 3 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dAdV = x.T\n",
    "array_print(dAdV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_a_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing $\\frac{\\partial l}{\\partial V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To compute $\\frac{\\partial l}{\\partial V}$, we simply multiply all of these partial derivatives we've calculated together, being careful to use matrix multiplication where necessary and elementwise multiplication where necessary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Computing $\\frac{\\partial l}{\\partial V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.01 -0.03 -0.02  0.04]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]]\n",
      "The dimensions are 3 rows and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dLdV = np.dot(dAdV, np.dot(dLdP * dPdC, dCdB) * dBdA)\n",
    "array_print(dLdV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that this has the same shape as $V$, which is what we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Updating the weights can now be done simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W -= dLdW\n",
    "V -= dLdV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Putting this all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's write some functions that train the neural net. The following just wraps around what we've already done, running one batch through the neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def learn(V, W, x_batch, y_batch):\n",
    "    # forward pass\n",
    "    A = np.dot(x_batch,V)\n",
    "    B = sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = sigmoid(C)\n",
    "    \n",
    "    # loss\n",
    "    L = 0.5 * (y_batch - P) ** 2\n",
    "    \n",
    "    # backpropogation\n",
    "    dLdP = -1.0 * (y_batch - P)\n",
    "    dPdC = sigmoid(C) * (1-sigmoid(C))\n",
    "    dLdC = dLdP * dPdC\n",
    "    dCdW = B.T\n",
    "    dLdW = np.dot(dCdW, dLdC)\n",
    "    dCdB = W.T\n",
    "    dBdA = sigmoid(A) * (1-sigmoid(A))\n",
    "    dAdV = x_batch.T\n",
    "    dLdV = np.dot(dAdV, np.dot(dLdP * dPdC, dCdB) * dBdA)\n",
    "    \n",
    "    # update the weights\n",
    "    W -= dLdW\n",
    "    V -= dLdV\n",
    "    \n",
    "    return V, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Putting this all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, let's play around with some results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  loss\n",
      "0       0  0.23\n",
      "1      50  0.13\n",
      "2     100  0.05\n",
      "3     150  0.02\n",
      "4     200  0.01\n",
      "5     250  0.01\n",
      "6     300  0.00\n",
      "7     350  0.00\n",
      "8     400  0.00\n",
      "9     450  0.00\n",
      "10    500  0.00\n",
      "The data frame of the predictions this neural net produces is:\n",
      "    Actual  Predicted\n",
      "0     1.0       0.94\n",
      "1     0.0       0.06\n",
      "2     1.0       0.98\n",
      "3     0.0       0.02\n",
      "4     0.0       0.08\n",
      "5     0.0       0.03\n",
      "6     1.0       0.98\n",
      "7     1.0       0.95\n",
      "The accuracy of this trained neural net is 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, W = train_and_display(X, Y, 500, 4)\n",
    "accuracy_binary(X, Y, V, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MNIST Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To illustrate that this simple framework really does have all the power of neural nets, let's show that it can solve the MNIST problem, the classic digit recognition task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# \"Fetch\" the data\n",
    "\n",
    "mnist = fetch_mldata('MNIST original') \n",
    "X_mnist, Y_mnist = get_mnist_X_Y(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "train_prop = 0.9\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_mnist, Y_mnist, \n",
    "    test_size=1-train_prop, \n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  loss\n",
      "0       0  0.14\n",
      "1       1  0.12\n",
      "2       2  0.12\n",
      "3       3  0.10\n",
      "4       4  0.10\n",
      "5       5  0.10\n",
      "6       6  0.09\n",
      "7       7  0.09\n",
      "8       8  0.08\n",
      "9       9  0.08\n",
      "10     10  0.08\n",
      "Neural Net MNIST Classification Accuracy: 95.0 percent\n"
     ]
    }
   ],
   "source": [
    "V, W = train_and_display(X_train, Y_train, 10, 50)\n",
    "accuracy = accuracy_multiclass(X_test, Y_test, V, W)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST Illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Keep in mind we got this accuracy without any \"tricks\": no convolutions, no dropout, no learning rate tuning - in fact, no \"Deep Learning\", since we used only one hidden layer! This shows how far simply having a solid understanding of the mathematics underlying neural nets can get you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transitioning to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "So: we've seen that neural nets can be expressed as mathematical functions. When expressed this way, we can explicity calculate the derivative of each nested function in the neural net to ensure that the weights are updated in the correct way and that the neural net will \"learn\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "But, this involved a lot of steps for a simple neural net with just one hidden layer. If we want to build deeper neural nets, we're going to have to come up with a way of describing neural nets other than as just \"nested functions\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another way to understand neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that if we were going to code up a deeper net - let's say one with two hidden layers instead of one - we would be repeating some steps:\n",
    "\n",
    "In the middle of the net on the forwards pass, we would be passing the output of a layers through a matrix multiplication, and then through an activation function, twice.\n",
    "\n",
    "Similarly, on the backwards pass, we would twice be passing \"values\" backwards, first based on the derivative of the activation function, and then based on the input to the prior layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neuron illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here's an illustration of what is going on at each neuron in the net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neuron_illustration_backprop.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In addition, this same operation is happening at the same neuron in each \"layer\" of the net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Towards a new way of thinking of neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We might be able to think of neural nets as a series of \"layers\", each of which sends values forward to the layer in front of it and sends values backwards during the backwards pass using the process described on the prior slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Working backwards from the API we want, we could define a neural net as follows:\n",
    "\n",
    "```\n",
    "layer1 = Layer(args)\n",
    "layer2 = Layer(args)\n",
    "net = Net([layer1, layer2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# New net framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here's how we could define new neural nets:\n",
    "\n",
    "Neural nets are a series of layers that pass information forwards based on what they receive as <em>input</em> and pass information backwards based on what they receive as the <em>loss</em>.\n",
    "\n",
    "Let's code this up. For getting me started on this code, I'm indebted to this guy:\n",
    "\n",
    "<div class=\"visual\">\n",
    "    <img src=\"img/andersbll.png\">\n",
    "</div>\n",
    "\n",
    "[Anders' GitHub](https://github.com/andersbll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layers, loss_function):\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        prediction = X_next\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, prediction, Y):\n",
    "        \"\"\" Calculate the loss on the data and send the \n",
    "        result backwards through the net. \"\"\"\n",
    "        loss = self.loss_function(prediction, Y)\n",
    "        return self.loss_function(prediction, Y, bprop=True)\n",
    "\n",
    "    def backpropogate(self, loss):\n",
    "        \"\"\" Backpropogate the loss through the net. \"\"\"\n",
    "        loss_next = loss\n",
    "        for layer in reversed(self.layers):\n",
    "            loss_next = layer.bprop(loss_next)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, let's define a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def _setup(self, input_shape):\n",
    "        \"\"\" Setup layer with parameters that are unknown at __init__(). \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fprop(self, input):\n",
    "        \"\"\" Calculate layer output for given input (forward propagation). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        \"\"\" Calculate input gradient. \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(Layer):\n",
    "    \n",
    "    random_seed = None\n",
    "    \n",
    "    def __init__(self, n_neurons, activation_function):\n",
    "        self.n_neurons = n_neurons\n",
    "        self.activation_function = activation_function        \n",
    "        self.iteration = 0\n",
    "        self.weights_initialized = False\n",
    "\n",
    "    def fprop(self, layer_input):\n",
    "        self.layer_input = layer_input\n",
    "        np.random.seed(seed=self.random_seed)\n",
    "        if not self.weights_initialized:\n",
    "            self.W = np.random.normal(size=(self.layer_input.shape[1], self.n_neurons))\n",
    "            self.weights_initialized = True\n",
    "        self.activation_input = np.dot(layer_input, self.W)\n",
    "        return self.activation_function(self.activation_input, bprop=False)\n",
    "\n",
    "    def bprop(self, layer_gradient):\n",
    "        dOutdActivationInput = self.activation_function(self.activation_input, bprop=True)\n",
    "        dLayerInputdActivationInput = layer_gradient * dOutdActivationInput\n",
    "        dActivationOutputdActivationInput = self.layer_input.T\n",
    "        output_grad = np.dot(dLayerInputdActivationInput, self.W.T)\n",
    "        weight_update = np.dot(dActivationOutputdActivationInput, dLayerInputdActivationInput)\n",
    "        W_new = self.W - weight_update\n",
    "        self.W = W_new\n",
    "        self.iteration += 1\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll need to redefine our functions to have `bprop` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, bprop=False):\n",
    "    if bprop:\n",
    "        return sigmoid(x) * (1-sigmoid(x))\n",
    "    else:\n",
    "        return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mean_square_error(prediction, Y, bprop=False):\n",
    "    if bprop:\n",
    "        return -1.0 * (Y - prediction)\n",
    "    else:\n",
    "        return 0.5 * (Y - prediction) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer1 = FullyConnected(n_neurons=50, activation_function=sigmoid)\n",
    "layer2 = FullyConnected(n_neurons=10, activation_function=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nn_mnist = NeuralNetwork(\n",
    "    layers=[layer1, layer2],\n",
    "    loss_function=mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the indices of the points in the training set:\n",
    "np.random.seed(4)\n",
    "train_size = X_train.shape[0]\n",
    "indices = list(range(train_size))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_pass(net, x, y):\n",
    "    pred = net.forwardpass(x)\n",
    "    loss = net.loss(pred, y)\n",
    "    net.backpropogate(loss)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n",
      "37000\n",
      "40000\n",
      "17000\n",
      "6000\n",
      "43000\n"
     ]
    }
   ],
   "source": [
    "# Loop through every element in the training set: \n",
    "for index in indices[0:5000]:\n",
    "    if index % 1000 == 0:\n",
    "        print(index)\n",
    "    x = np.array(X_train[index], ndmin=2)\n",
    "    y = np.array(Y_train[index], ndmin=2)\n",
    "    neural_net_pass(nn_mnist, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net MNIST Classification Accuracy: 71.0 percent\n"
     ]
    }
   ],
   "source": [
    "P = nn_mnist.forwardpass(X_test)\n",
    "preds = [np.argmax(x) for x in P]\n",
    "actuals = [np.argmax(x) for x in Y_test]\n",
    "\n",
    "accuracy = sum(np.array(preds) == np.array(actuals)) * 1.0 / len(preds)\n",
    "print(\"Neural Net MNIST Classification Accuracy:\", round(accuracy, 3) * 100, \"percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "47px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
