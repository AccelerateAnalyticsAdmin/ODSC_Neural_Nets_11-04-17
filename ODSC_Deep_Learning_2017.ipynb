{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".expo {\n",
       "  line-height: 150%;\n",
       "}\n",
       "\n",
       ".visual {\n",
       "  width: 400px;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"\"\"\n",
    "<style>\n",
    ".expo {\n",
    "  line-height: 150%;\n",
    "}\n",
    "\n",
    ".visual {\n",
    "  width: 400px;\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning from Scratch using Python \n",
    "\n",
    "Seth Weidman\n",
    "\n",
    "11/04/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# To begin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nah..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "This talk will have two parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 1: Neural Nets from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll implement a basic neural net with one hidden layer, from scratch, and use mathematical principles to get the backpropogation right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll show that this same framework can be used to learn MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 2: Transitioning to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll transition to Deep Learning by changing our mental model of neural nets to be that they contain \"layers\" which pass information backwards and forwards between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We'll show how this framework can be used to construct arbitrarily deep neural networks, and show how these can learn MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Neural Nets from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "We've all seen diagrams like the following in the context of neural nets:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 1: Neural Nets from Scratch\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_v3.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Many don't fully understand what is going on in this diagram. This talk will attempt to rectify that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why neural nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Let's suppose we need a function that  that can learn a complicated relationship between inputs and outputs:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df, X, Y = generate_x_y()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Neural Nets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X1  X2  X3  y\n",
       "0   1   0   0  1\n",
       "1   0   1   0  0\n",
       "2   1   1   0  1\n",
       "3   0   0   0  0\n",
       "4   1   0   1  0\n",
       "5   0   0   1  0\n",
       "6   1   1   1  1\n",
       "7   0   1   1  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "So: we have eight observations, and a complex relationship between inputs and outputs. Now, let's build a neural net that can \"learn\" this relationship.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Neural Nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "How to even begin? Well, let's look at this from as high a level as possible and then progressively dive deeper. First, we want a function $N$ that--based on the data from before--maps inputs to outputs properly, that is:\n",
    "</div>\n",
    "\n",
    "$$ N(1, 0, 0) = 1 $$\n",
    "$$ N(0, 1, 0) = 1 $$\n",
    "$$ N(1, 1, 0) = 1 $$\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "First, let's observe that logistic regression can't do this. That is, there are no parameters $b$, $w_1$, $w_2$, and $w_3$ such that:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$N(x_1, x_2, x_3) = \\frac{1}{1 + e^{b + w_1 * x_1 + w_2 * x_2 + w_3 * x_3}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "This is true for the same reason that logistic regression cannot learn XOR. Our problem is a three dimensional problem since we have three features, but you can easily see in two dimensions that the space is not linearly separable:\n",
    "</div>\n",
    "\n",
    "<div class=\"visual\">\n",
    "  <img src=\"img/xor.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Of course, we *could* manually do feature engineering...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... but who likes feature engineering???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "Let's have the computer do the feature engineering for us, via a neural net learning hidden features!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's make a prediction using a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our goal will be to:\n",
    "\n",
    "* Start with our three original features.\n",
    "* Transform them into four \"intermediate features\" using logistic-regression-like transformation.\n",
    "* Take these four \"intermediate features\" and use _them_ to predict our final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we transform our original three features into an intermediate, or hidden layer? Let's call our intermediate features $a_1$, $a_2$, $a_3$, and $a_4$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want $a_1$, for example, to be a linear combination of $x_1$, $x_2$, and $x_3$ - that is, we want some weights $v_{11}$, $v_{12}$, and $v_{13}$ so that:\n",
    "\n",
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "\n",
    "and similarly for $a_2$, $a_3$, and $a_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A way to concisely express this is to define your features as a vector\n",
    "\n",
    "$$ X = \\begin{bmatrix}x_1 & x_2 & x_3\\end{bmatrix} $$\n",
    "\n",
    "This should be intuitive, since $X$ is already a row in your data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, multiplying this vector by a matrix $V$:\n",
    "\n",
    "$$ V = \\begin{bmatrix}v_{11} & v_{12} & v_{13} & v_{14} \\\\\n",
    "                      v_{21} & v_{22} & v_{23} & v_{24} \\\\\n",
    "                      v_{31} & v_{32} & v_{33} & v_{34}\n",
    "                      \\end{bmatrix} $$\n",
    "\n",
    "gives us what we want, since \"$A = X * V$\" is equivalent to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ a_1 = x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} $$\n",
    "$$ a_2 = x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} $$\n",
    "$$ a_3 = x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} $$\n",
    "$$ a_4 = x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} $$\n",
    "\n",
    "which is what we want in order to get four intermediate features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's code this up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[1 0 0]]\n",
      "The dimensions are 1 row and 3 columns\n"
     ]
    }
   ],
   "source": [
    "x = np.array(X[0], ndmin=2)\n",
    "array_print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: feeding features to the \"intermediate\" or \"hidden\" layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 1.82  0.57  1.55  0.1 ]\n",
      " [-1.15 -0.91 -1.28  0.7 ]\n",
      " [ 0.25 -1.    0.66  1.1 ]]\n",
      "The dimensions are 3 rows and 4 columns\n"
     ]
    }
   ],
   "source": [
    "V = np.random.randn(3, 4)\n",
    "array_print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 1.82  0.57  1.55  0.1 ]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "A = np.dot(x, V)\n",
    "array_print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_first_layer.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We're going to use a classic, easy-to-understand activation function, though one that is not often used in cutting-edge applications: the sigmoid function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ B = \\sigma(A) $$ or\n",
    "\n",
    "$$ b_1 = \\sigma(a_1) $$\n",
    "$$ b_2 = \\sigma(a_2) $$\n",
    "$$ b_3 = \\sigma(a_3) $$\n",
    "$$ b_4 = \\sigma(a_4) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2: feeding these intermediate features through an \"activation function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.86  0.64  0.82  0.52]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "B = sigmoid(A)\n",
    "array_print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_first_sigmoid.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll multiply these \"sigmoided\" results by another matrix $W$ to get a single output. Since we want to transform 4 features down into 1, we can use a 4 x 1 matrix:\n",
    "\n",
    "$$ W = \\begin{bmatrix}w_{11} \\\\\n",
    "                      w_{21} \\\\\n",
    "                      w_{31} \\\\\n",
    "                      w_{41}\n",
    "                      \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And since we want the result to be:\n",
    "\n",
    "$$ c_1 = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is equivalent to writing:\n",
    "\n",
    "$$ C = B * W $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or:\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "c_1 \\end{bmatrix} = \n",
    "\\begin{bmatrix}b_1 &\n",
    "                  b_2 &\n",
    "                  b_3 &\n",
    "                  b_4\n",
    "                  \\end{bmatrix} * \n",
    "\\begin{bmatrix}w_{11} \\\\\n",
    "               w_{21} \\\\\n",
    "               w_{31} \\\\\n",
    "               w_{41}\n",
    "               \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 3: use these intermediate features as a linear combination to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can simply code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.42]\n",
      " [-0.01]\n",
      " [ 0.14]\n",
      " [-0.95]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(4, 1)\n",
    "array_print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.02]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "C = np.dot(B, W)\n",
    "array_print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_second_layer.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 4: sigmoid this to make a final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we want:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ p_1 = \\sigma(c_1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we can simply code this up as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.49]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "P = sigmoid(C)\n",
    "array_print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_final_prediction.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 5: compute the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we'll compute mean squared error loss:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = \\frac{1}{2}(y - P)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.13]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "y = np.array(Y[0], ndmin=2)\n",
    "L = 0.5 * (y - P) ** 2\n",
    "array_print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"visual\">\n",
    "<img src='img/neural_net_4_loss.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have made our prediction and computed our loss, $L$. Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall: each \"step\" is just a function applied to some input that results in some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we write out what we just did in terms of mathematical functions, we could write it as:\n",
    "\n",
    "\\begin{align}\n",
    "A &= a(x, V) \\\\\n",
    "B &= b(A) \\\\\n",
    "C &= c(B, W) \\\\\n",
    "P &= p(C) \\\\\n",
    "L &= l(P)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, say we have a neural net with just one hidden layer. We could write the loss of a neural net on a given observation $ x $ as:\n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, we _want_ to change the weights in such a way that the loss will be reduced during the next iteration. The equations:\n",
    "\n",
    "$$ W = W - \\frac{\\partial l}{\\partial W}$$\n",
    "\n",
    "$$ V = V - \\frac{\\partial l}{\\partial V}$$\n",
    "\n",
    "do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that this \"makes sense\":\n",
    "\n",
    "* If $\\frac{\\partial l}{\\partial W}$ is a positive number, then we want to _decrease_ the weight, since increasing the weight would _increase_ our loss. That is exactly what the equation $ W = W - \\frac{\\partial l}{\\partial W}$ does.\n",
    "* Similarly, if $\\frac{\\partial l}{\\partial W}$ is a negative number, then we want to _increase_ the weight, since increasing the weight would _decrease_ our loss. In both cases, the equation $ W = W - \\frac{\\partial l}{\\partial W}$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we want to make our neural net smarter by updating its weights. We've see that to do that, we need to compute $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial V}$. How do we do this?\n",
    "\n",
    "Well, we know that \n",
    "\n",
    "$$ L = l(p(c(b(a(x, V)), W))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our good friend the chain rule tells us that: \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial W}  $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial V} = \\frac{\\partial l}{\\partial P} * \\frac{\\partial p}{\\partial C} * \\frac{\\partial c}{\\partial B} * \\frac{\\partial b}{\\partial A} * \\frac{\\partial a}{\\partial V}  $$\n",
    "\n",
    "Each one of these partial derivatives turns out to be simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, let's compute:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial P} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since \n",
    "\n",
    "$$ L = l(P) = \\frac{1}{2}(y - P)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then:\n",
    "\n",
    "$$ \\frac{\\partial l}{\\partial P} = -(y - P)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.51]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "dLdP = -(y - P)\n",
    "array_print(dLdP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_loss_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, let's compute:\n",
    "\n",
    "$$ \\frac{\\partial p}{\\partial C} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that:\n",
    "\n",
    "$$ P = \\begin{bmatrix} p_1 \\end{bmatrix} = p(c) = \\sigma(c) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A digression on the sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) * (1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if\n",
    "\n",
    "$$ p(c) = \\sigma(c) $$\n",
    "\n",
    "then\n",
    "\n",
    "$$ p'(c) = \\sigma(c) * (1 - \\sigma(c)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.25]]\n",
      "The dimensions are 1 row and 1 column\n"
     ]
    }
   ],
   "source": [
    "dPdC = sigmoid(C) * (1-sigmoid(C))\n",
    "array_print(dPdC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_prediction_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next we want to compute:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "C &= \\begin{bmatrix} c_1 \\end{bmatrix} \\\\ \n",
    "&= c(W) \\\\\n",
    "&= w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now recall that by \n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} $$\n",
    "\n",
    "we mean:\n",
    "\n",
    "$$ \\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But since \n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ \\frac{\\partial c}{\\partial w_{11}} $, for example, is just $b_1$, $ \\frac{\\partial c}{\\partial w_{21}} $ is just $b_2$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus,\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is just $ B^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So, coding this up is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.86]\n",
      " [ 0.64]\n",
      " [ 0.82]\n",
      " [ 0.52]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dCdW = B.T\n",
    "array_print(dCdW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that this has the same dimensions as `W`, which is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing $\\frac{\\partial L}{\\partial W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now computing $\\frac{\\partial L}{\\partial W}$ is simply a matter of doing the matrix multiplications, which again, by the chain rule, will actually cause the weights to be updated in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.109]\n",
      " [-0.081]\n",
      " [-0.104]\n",
      " [-0.066]]\n",
      "The dimensions are 4 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dLdW = np.dot(dCdW, dLdP * dPdC)\n",
    "array_print(dLdW, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropogation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By the same logic that we applied in Step 3, since:\n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$\n",
    "\n",
    "we have:\n",
    "\n",
    "$$ \\frac{\\partial c}{\\partial W} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial w_{11}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{21}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{31}} \\\\\n",
    "                  \\frac{\\partial c}{\\partial w_{41}}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}b_1 \\\\\n",
    "                  b_2 \\\\\n",
    "                  b_3 \\\\\n",
    "                  b_4\n",
    "                  \\end{bmatrix} = B^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And again, since:\n",
    "\n",
    "$$ c(W) = w_{11} * b_1 + w_{21} * b_2 + w_{31} * b_3 + w_{41} * b_4 $$\n",
    "\n",
    "We have:\n",
    "    \n",
    "$$ \\frac{\\partial c}{\\partial B} =\n",
    "\\begin{bmatrix}\\frac{\\partial c}{\\partial b_1} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_2} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_3} \\\\\n",
    "                  \\frac{\\partial c}{\\partial b_4}\n",
    "                  \\end{bmatrix} = \\begin{bmatrix}w_{11} \\\\\n",
    "                  w_{21} \\\\\n",
    "                  w_{31} \\\\\n",
    "                  w_{41}\n",
    "                  \\end{bmatrix} = W^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So coding this up simply gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.42 -0.01  0.14 -0.95]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dCdB = W.T\n",
    "array_print(dCdB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_c_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next, we want to compute:\n",
    "\n",
    "$$ \\frac{\\partial b}{\\partial A} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But since:\n",
    "\n",
    "$$ B = b(A) = \\sigma(A) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is really just shorthand for:\n",
    "\n",
    "$$ B = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ b_4 \\end{bmatrix} = \\begin{bmatrix} \\sigma(a_1) \\\\ \\sigma(a_2) \\\\ \\sigma(a_3) \\\\ \\sigma(a_4) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since we know that:\n",
    "    \n",
    "$$ \\sigma'(A) = \\sigma(A) * (1 - \\sigma(A)) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then:\n",
    "    \n",
    "$$ \\frac{\\partial b}{\\partial A} = \\begin{bmatrix} \\sigma(a_1) * (1 - \\sigma(a_1) \\\\ \n",
    "\\sigma(a_2) * (1 - \\sigma(a_2) \\\\ \n",
    "\\sigma(a_3) * (1 - \\sigma(a_3) \\\\\n",
    "\\sigma(a_4) * (1 - \\sigma(a_4) \\end{bmatrix} = \\sigma(A) * (1 - \\sigma(A))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[ 0.12  0.23  0.14  0.25]]\n",
      "The dimensions are 1 row and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dBdA = sigmoid(A) * (1-sigmoid(A))\n",
    "array_print(dBdA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_b_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, we want to compute the most involved of our partial derivatives:\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial V} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling that:\n",
    "\n",
    "$$ a(X, V) = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But $ a(X, V) $ is itself shorthand for the equations:\n",
    "\n",
    "$$ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $$\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$\n",
    "$$ x_1 * v_{13} + x_2 * v_{23} + x_3 * v_{33} = a_3 $$\n",
    "$$ x_1 * v_{14} + x_2 * v_{24} + x_3 * v_{34} = a_4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So $ \\frac{\\partial a}{\\partial V} $ is really shorthand for:\n",
    "\n",
    "$$ \\begin{bmatrix}\\frac{\\partial a}{\\partial v_{11}} & \\frac{\\partial a}{\\partial v_{12}} & \\frac{\\partial a}{\\partial v_{13}} & \\frac{\\partial a}{\\partial v_{14}} \\\\\n",
    "\\frac{\\partial a}{\\partial v_{21}} & \\frac{\\partial a}{\\partial v_{22}} & \\frac{\\partial a}{\\partial v_{23}} & \\frac{\\partial a}{\\partial v_{24}} \\\\\n",
    "\\frac{\\partial a}{\\partial v_{31}} & \\frac{\\partial a}{\\partial v_{32}} & \\frac{\\partial a}{\\partial v_{33}} & \\frac{\\partial a}{\\partial v_{34}} \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But, note that focusing on just $a_1$ for example:\n",
    "\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{11}} = x_1 $$\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{21}} = x_2 $$\n",
    "$$ \\frac{\\partial a_1}{\\partial v_{31}} = x_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since again,\n",
    "\n",
    "$$ x_1 * v_{11} + x_2 * v_{21} + x_3 * v_{31} = a_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "whereas for $a_2$ and $a_3$\n",
    "\n",
    "$$ \\frac{\\partial a_2}{\\partial v_{11}} = 0 $$\n",
    "$$ \\frac{\\partial a_3}{\\partial v_{11}} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since, for example:\n",
    "\n",
    "$$ x_1 * v_{12} + x_2 * v_{22} + x_3 * v_{32} = a_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So if we write: \n",
    "    \n",
    "$$ A = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ a_3 \\\\ a_4 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then $\\frac{\\partial a}{\\partial V}$ ends up being:\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial V} = \\begin{bmatrix}\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} &\n",
    "   \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which in terms of the matrix multiplication that results is the same as writing just:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{\\partial a}{\\partial V} = x^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Backpropogation - step 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which is of course easy to code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[1]\n",
      " [0]\n",
      " [0]]\n",
      "The dimensions are 3 rows and 1 column\n"
     ]
    }
   ],
   "source": [
    "dAdV = x.T\n",
    "array_print(dAdV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where are we"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neural_net_4_a_grad.png'>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computing $\\frac{\\partial l}{\\partial V}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To compute $\\frac{\\partial l}{\\partial V}$, we simply multiply all of these partial derivatives we've calculated together, being careful to use matrix multiplication where necessary and elementwise multiplication where necessary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing $\\frac{\\partial l}{\\partial V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The array:\n",
      " [[-0.01  0.   -0.    0.03]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]]\n",
      "The dimensions are 3 rows and 4 columns\n"
     ]
    }
   ],
   "source": [
    "dLdV = np.dot(dAdV, np.dot(dLdP * dPdC, dCdB) * dBdA)\n",
    "array_print(dLdV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that this has the same shape as $V$, which is what we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Updating the weights can now be done simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "W -= dLdW\n",
    "V -= dLdV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Putting this all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's write some functions that train the neural net. The following just wraps around what we've already done, running one batch through the neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def learn(V, W, x_batch, y_batch):\n",
    "    # forward pass\n",
    "    A = np.dot(x_batch,V)\n",
    "    B = sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = sigmoid(C)\n",
    "    \n",
    "    # loss\n",
    "    L = 0.5 * (y_batch - P) ** 2\n",
    "    \n",
    "    # backpropogation\n",
    "    dLdP = -1.0 * (y_batch - P)\n",
    "    dPdC = sigmoid(C) * (1-sigmoid(C))\n",
    "    dLdC = dLdP * dPdC\n",
    "    dCdW = B.T\n",
    "    dLdW = np.dot(dCdW, dLdC)\n",
    "    dCdB = W.T\n",
    "    dBdA = sigmoid(A) * (1-sigmoid(A))\n",
    "    dAdV = x.T\n",
    "    dLdV = np.dot(dAdV, np.dot(dLdP * dPdC, dCdB) * dBdA)\n",
    "    \n",
    "    # update the weights\n",
    "    W -= dLdW\n",
    "    V -= dLdV\n",
    "    \n",
    "    return V, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting this all together, Pt. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now a function to initialize the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_weights(num_hidden=4):\n",
    "    '''\n",
    "    Randomly initializes weights\n",
    "    '''\n",
    "    np.random.seed(1104)\n",
    "    V = np.random.randn(3, num_hidden)\n",
    "    W = np.random.randn(num_hidden, 1)\n",
    "    return V, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting this all together, Pt. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And one to shuffle the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_x_y(X, Y):\n",
    "    '''\n",
    "    Each array must be two dimensional\n",
    "    '''\n",
    "    np.random.seed(1104)\n",
    "    train_size = X.shape[0]\n",
    "    indices = list(range(train_size))\n",
    "    np.random.shuffle(indices)\n",
    "    return X[indices], Y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting this all together, Pt. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This runs the data through the net, one observation at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def one_epoch(X, Y, V, W):\n",
    "    '''\n",
    "    Run one epoch an element at a time through the net.\n",
    "    '''\n",
    "    for index in range(X.shape[0]):\n",
    "        x_batch = np.array(X[index], ndmin=2)\n",
    "        y_batch = np.array(Y[index], ndmin=2)\n",
    "        learn(V, W, x_batch, y_batch)\n",
    "    return V, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting this all together, Pt. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This takes in a batch of observations and makes predictions using the latest learned weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict(x_batch, V, W):\n",
    "    '''\n",
    "    Make a prediction given a batch of observations and the weights.\n",
    "    '''\n",
    "    A = np.dot(x_batch,V)\n",
    "    B = sigmoid(A)\n",
    "    C = np.dot(B,W)\n",
    "    P = sigmoid(C)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting this all together, Pt. 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This calculates the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def loss(prediction, actual, print_loss=False):\n",
    "    '''\n",
    "    Calculate the loss.\n",
    "    '''\n",
    "    df = pd.DataFrame(data=np.concatenate([prediction, actual], axis=1),\n",
    "                      columns=['predictions', 'actuals'])\n",
    "    df['loss'] = 0.5 * (df['predictions'] - df['actuals']) ** 2\n",
    "    return df['loss'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting this all together, Pt. 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, this function trains the net for a number of epochs and outputs a data frame illustrating the training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train(X, Y, V, W, epochs=100):\n",
    "    '''\n",
    "    Train the net for a number of epochs.\n",
    "    '''\n",
    "    losses = []\n",
    "    epochs_list = []\n",
    "    for i in range(epochs+1):\n",
    "        V, W = one_epoch(X, Y, V, W)\n",
    "        if i % (epochs / 10) == 0:\n",
    "            preds = predict(X, V, W)\n",
    "            loss_epoch = loss(preds, Y)\n",
    "            epochs_list.append(i)\n",
    "            losses.append(loss_epoch)\n",
    "    return pd.DataFrame({'epoch' : epochs_list, \n",
    "                         'loss' : losses})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting this all together, Pt. 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This function wraps around much of what we've done so far, training the net and displaying the result of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_display(X, Y, num_epochs=1000, num_hidden=8):\n",
    "    X, Y = shuffle_x_y(X, Y)\n",
    "    V, W = initialize_weights(num_hidden=num_hidden)\n",
    "    df = train(X, Y, V, W, num_epochs)\n",
    "    df_print(df)\n",
    "    return V, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting this all together, Pt. 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And this monster of a function computes the accuracy of the net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(X, Y, V, W):\n",
    "    \n",
    "    def _df_actual_predicted(X, Y, V, W):\n",
    "        return pd.DataFrame(np.round(np.concatenate([Y, predict(X, V, W)], axis=1), 2),\n",
    "                            columns=[\"Actual\", \"Predicted\"])\n",
    "\n",
    "    df = _df_actual_predicted(X, Y, V, W)\n",
    "    print(\"The data frame of the predictions this neural net produces is:\\n\",\n",
    "          df)\n",
    "    df['Prediction'] = df['Predicted'] > 0.5\n",
    "\n",
    "    def _correct_prediction(row):\n",
    "        return bool(row['Actual']) == row['Prediction']\n",
    "    \n",
    "    df['Correct'] = df.apply(lambda x: _correct_prediction(x), axis=1)\n",
    "        \n",
    "    print(\"The accuracy of this trained neural net is\", \n",
    "          df['Correct'].sum() / len(df['Correct']))\n",
    "    \n",
    "    return df['Correct'].sum() / len(df['Correct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting this all together, Pt. 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, let's play around with some results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    epoch  loss\n",
      "0       0  0.22\n",
      "1     100  0.05\n",
      "2     200  0.04\n",
      "3     300  0.04\n",
      "4     400  0.03\n",
      "5     500  0.03\n",
      "6     600  0.02\n",
      "7     700  0.02\n",
      "8     800  0.02\n",
      "9     900  0.02\n",
      "10   1000  0.01\n",
      "The data frame of the predictions this neural net produces is:\n",
      "    Actual  Predicted\n",
      "0     1.0       0.67\n",
      "1     0.0       0.20\n",
      "2     1.0       1.00\n",
      "3     0.0       0.00\n",
      "4     0.0       0.13\n",
      "5     0.0       0.00\n",
      "6     1.0       0.99\n",
      "7     1.0       0.76\n",
      "The accuracy of this trained neural net is 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V, W = train_and_display(X, Y, 1000, 8)\n",
    "accuracy(X, Y, V, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transitioning to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "So: we've seen that neural nets can be expressed as mathematical functions. When expressed this way, we can explicity calculate the derivative of each nested function in the neural net to ensure that the weights are updated in the correct way and that the neural net will \"learn\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"expo\">\n",
    "But, this involved a lot of steps for a simple neural net with just one hidden layer. If we want to build deeper neural nets, we're going to have to come up with a way of describing neural nets other than as just \"nested functions\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Another way to understand neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that if we were going to code up a deeper net - let's say one with two hidden layers instead of one - we would be repeating some steps:\n",
    "\n",
    "* In the middle of the net on the forwards pass, we would be passing the output of a layers through a matrix multiplication, and then through an activation function, twice.\n",
    "* Similarly, on the backwards pass, we would be passing \"values\" backwards, first based on the derivative of the activation function, and then based on the input to the prior layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neuron illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here's an illustration of what is going on at each neuron in the net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"visual\">\n",
    "    <img src='img/neuron_illustration_backprop.png'>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "47px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
